input {

	# start input Agent less for syslog
	tcp {
		host => "0.0.0.0"
		port => 6514
		type => syslog
		#ssl_enable => true # Enable SSL/TLS
		#ssl_cert => <Certificate Path>
		#ssl_key => <CertificateKey Path>
		#ssl_key_passphrase => <Key PassPhrase>
		#ssl_verify => false #Flag to validate certificate with CA (Should be false for Self signed Certificate)
	}

	udp {
		host => "0.0.0.0"
		port => 5140
		type => syslog
	}

}

filter {

	ruby {
	    code => "require 'base64';
    	    event.set('auth_token',Base64.encode64('admin:admin'))"
	}
	mutate {
		add_field => {
			"[@metadata][ingestIntoJarvis]" => "false"
		}
	}
	mutate { gsub => [ "auth_token", "\n", "" ] }
	if [type] == "tenant_id" {
		mutate {
			uppercase => [ "message" ]
		}
	}
	else if [type] == "syslog" {

	   if ( ([@metadata][ingestIntoJarvis] == "false") ) {

	       mutate { gsub => [
                                  "message", "\\'", "'",
                                  "message", "[\\]", "/",
                                  "message", "\/\"", "\"",
                                  "message", "\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]", ""
                                 ]
                        }

                json {
                        source => "message"
                }

                if [tag] == "eventlogs" {
                        mutate {
                                lowercase => [ "message" ]
                                uppercase => [ "tenantid" ]
                                add_field => [ "temp_token" ,  "%{tenantid}" ]
                        }
                }
                else {
                        mutate {
                                uppercase => [ "tenant_id" ]
                                add_field => [ "temp_token" ,  "%{tenant_id}" ]
                        }
                }

	   }
	   else {

		mutate { gsub => [
				  "message", "\\'", "'",
	     		          "message", "[\\]", "/",
				  "message", "\/\"", "\"",
               			  "message", "\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]", ""
				 ]
			}

		json {
			source => "message"
		}

		if [tag] == "eventlogs" {
			mutate {
				lowercase => [ "message" ]
				uppercase => [ "tenantid" ]
				add_field => [ "temp_token" ,  "%{tenantid}" ]
			}
		}
		else {
			mutate {
				uppercase => [ "tenant_id" ]
				add_field => [ "temp_token" ,  "%{tenant_id}" ]
			}
		}


		# RAW MESSAGE storage can be enabled for debugging by uncommenting the below three lines.
		# This should NOT BE enabled on production as it almost doubles the storage
		#mutate {
		#	 add_field => [ "raw_msg" ,  "%{message}" ]
		#}

		json {
			source => "message"
			target => "body"
		}

		if (([body][type_category] == "metrics") or ([body][type_category] == "events") or ([body][type_category] == "logs") or ([body][type_category] == "alarms") or ([body][type_category] == "inventory")) {
			mutate {
				add_field => {
					"temp_group" => "%{[body][documents][group]}"
					"temp_tags" => "%{[body][documents][tags]}"
					"temp_services_impacted" => "%{[body][documents][services_impacted]}"
				}
			}

			mutate {
				gsub => [
					'temp_group', ",", '","',
					'temp_group', '" ', '"',
					'temp_group', ' "', '"',
					'temp_tags', ",", '","',
					'temp_tags', '" ', '"',
					'temp_tags', ' "', '"',
					'temp_services_impacted', ",", '","',
					'temp_services_impacted', '" ', '"',
					'temp_services_impacted', ' "', '"'
				]
			}
		}

		#===========================#
		# Start syslog parsing
		#===========================#
			date {
				match => [ "[body][syslog_timestamp]", "YYYY-MM-dd'T'HH:mm:ss.SSSSSSZ", "ISO8601", "MMM dd HH:mm:ss", "YYYY MMM d HH:mm:ss" ]
				timezone => "UTC"
				target => "timestamp"
			}

			mutate {
				add_field => {
					"temp_timestamp" => "%{timestamp}"
					"temp_received_timestamp" => "%{@timestamp}"
				}
			}

			mutate {
				gsub => [
					"temp_timestamp", "\.\d{3}Z$", "Z",
					"temp_received_timestamp", "\.\d{3}Z$", "Z",
					"[body][syslog_pid]" , "-", "1",
					"[body][syslog_message]", "\"", ""
				]
			}

			mutate {
				add_field => {
					"audit_msg" => "%{[body][syslog_message]}"
					"syslog_prog" => "%{[body][syslog_program]}"
				}
			}

			# Only for Oracle Audit trail as Syslog
			if [audit_msg] =~ / Audit/ {
				mutate {
					remove_field => [ "[header][doc_type_id]" ]
					remove_field => [ "[body][logtype]" ]
					add_field => [ "[body][tags]" ,  "syslog" ]
				}

				# Replacing $ with S as logstash parse failure due to this
				mutate {
					gsub => [ "audit_msg", "\$", "S" ]
				}
				mutate {
					gsub => [ "audit_msg" , "\"", "'" ]
				}
				grok {
					patterns_dir => "/logcollector_config/patterns/"
					match => [ "audit_msg", "%{ORACLE_AUDIT_P1}", "audit_msg", "%{ORACLE_AUDIT_P2}", "audit_msg", "%{ORACLE_AUDIT_P3}", "audit_msg", "%{ORACLE_AUDIT_P4}", "audit_msg", "%{ORACLE_AUDIT_P5}" ]
				}

				mutate {
					gsub => [ "action" , "'", "" ]
				}
				# Added for Oracle Action Names
				mutate {
					 add_field => [ "temp_token" ,  "%{action}" ]
					 add_field => [ "action_name", "UNKNOWN" ]
				}

				translate {
					dictionary_path => "/logcollector_config/dictionary/event_oracle.yml"
					field => [ "temp_token" ]
					refresh_interval => 3600
					periodic_flush => true
				}
				# Check if key value pair exists in 'yml' file
				if ([translation])  {
					mutate {
						update => [ "action_name", "%{translation}" ]
					}
				}
				mutate {
					  remove_field => [ "temp_token" , "translation" ]
				}

				# jarvis 2.0 header & body
				mutate {
					add_field => {
						"jarvis_message" => '{
							"documents": [ {
								"header": {
									"product_id": "ao",
									"tenant_id": "%{[body][tenant_id]}",
									"doc_type_id": "itoa_logs_oracle_audit",
									"doc_type_version": "1"
								},
								"body": [ {
									"timestamp": "%{temp_timestamp}",
									"received_timestamp": "%{temp_received_timestamp}",
									"logtype": "oracle_audit",
									"tags": ["oracle_audit", "%{[body][tags]}"],
									"syslog_timestamp": "%{temp_timestamp}",
									"syslog_pri": "%{[body][syslog_pri]}",
									"syslog_message": "%{[body][syslog_message]}",
									"host": "%{[body][host]}",
									"syslog_severity": "%{[body][syslog_severity]}",
									"syslog_facility": "%{[body][syslog_facility]}",
									"syslog_severity_code": %{[body][syslog_severity_code]},
									"syslog_facility_code": %{[body][syslog_facility_code]},
									"syslog_program": "%{[body][syslog_program]}",
									"syslog_pid": "%{[body][syslog_pid]}",
									"syslog_hostname": "%{[body][syslog_hostname]}",
									"syslog_priority": %{[body][syslog_priority]},
									"length": %{length},
									"sessionid": "%{session_id}",
									"entryid": "%{entry_id}",
									"user": "%{dbuser}",
									"action": "%{action}",
									"action_name": "%{action_name}",
									"returncode": "%{return_code}",
									"logoffspread": "%{logoffspread}",
									"logoffslread": "%{logoffslread}",
									"logoffslwrite": "%{logoffslwrite}",
									"logoffsdead": "%{logoffsdead}",
									"sessioncpu": "%{session_cpu}",
									"dbuserpreviliges": "%{dbuser_previliges}",
									"clientuser": "%{client_user}",
									"clientterminal": "%{client_terminal}",
									"status": "%{status}",
									"dbid": "%{dbid}",
									"statement": "%{statement}",
									"userhost": "%{userhost}",
									"comments": "%{commentstext}",
									"osuserid": "%{osuserid}",
									"privused": "%{privused}"
								} ]
							} ]
						}'
					}
				}
			}
			# For Docker Parsing
			else if [syslog_prog] =~ /(?i)dockerd/ {
				mutate {
					remove_field => [ "[header][doc_type_id]" ]
					remove_field => [ "[body][logtype]" ]
					add_field => [ "[body][tags]" ,  "syslog" ]
				}
				# Start inserting the docker data in Docker Index not syslog index
				mutate {
					add_field => [ "[header][doc_type_id]" ,  "itoa_logs_docker" ]
				}

				grok {
				    patterns_dir => "/logcollector_config/patterns/"
					match => [ "audit_msg", "%{DOCKER_SYSLOG_MSG}" ]
				}

				grok {
					patterns_dir => "/logcollector_config/patterns/"
					match => [ "doc_time", "%{TIMESTAMP_ISO8601:doc_timestamp}" ]
				}
				date {
					match => [ "doc_timestamp", "yyyy-MM-dd'T'HH:mm:ss.SSSSSSSSSZZ", "yyyy-MM-dd'T'HH:mm:ssZZ" ]
					timezone => "UTC"
					target => "doc_time"
				}

				mutate {
					add_field => {
						"temp_doc_time" => "%{doc_time}"
					}
				}

				mutate {
					gsub => [
						"temp_doc_time", "\.\d{3}Z$", "Z"
					]
				}


				# GET, PUT, POST, DELETE, HEAD
				if ([msg] =~ /(?i)GET /) or ([msg] =~ /(?i)PUT /) or ([msg] =~ /(?i)POST /) or ([msg] =~ /(?i)DELETE /) or ([msg] =~ /(?i)HEAD /) {
					grok {
						patterns_dir => "/logcollector_config/patterns/"
						match => { "msg" =>
							[
								"%{DOCKER_SPLIT_MSG_P1}",
								"%{DOCKER_SPLIT_MSG_P2}",
								"%{DOCKER_SPLIT_MSG_P3}",
								"%{DOCKER_SPLIT_MSG_P4}",
								"%{DOCKER_SPLIT_MSG_P5}",
								"%{DOCKER_SPLIT_MSG_P6}"
							]
						}
					}

					grok {
						patterns_dir => "/logcollector_config/patterns/"
						match => [ "urlpath", "%{WORD:cont_id}" ]
					}

					mutate {
						add_field => {
							"[body][container_id]" => "%{cont_id}"
						}
					}
				}
				# jarvis 2.0 header & body
				mutate {
					add_field => {
						"jarvis_message" => '{
							"documents": [ {
								"header": {
									"product_id": "ao",
									"tenant_id": "%{[body][tenant_id]}",
									"doc_type_id": "itoa_logs_docker",
									"doc_type_version": "1"
								},
								"body": [ {
									"timestamp": "%{temp_doc_time}",
									"received_timestamp": "%{temp_received_timestamp}",
									"logtype": "docker",
									"tags": ["docker, %{[body][tags]}"],
									"syslog_timestamp": "%{temp_timestamp}",
									"syslog_pri": "%{[body][syslog_pri]}",
									"syslog_message": "%{[body][syslog_message]}",
									"host": "%{[body][host]}",
									"syslog_severity": "%{[body][syslog_severity]}",
									"syslog_facility": "%{[body][syslog_facility]}",
									"syslog_severity_code": %{[body][syslog_severity_code]},
									"syslog_facility_code": %{[body][syslog_facility_code]},
									"syslog_program": "%{[body][syslog_program]}",
									"syslog_pid": "%{[body][syslog_pid]}",
									"syslog_hostname": "%{[body][syslog_hostname]}",
									"syslog_priority": %{[body][syslog_priority]},
									"log_level": "%{log_level}",
									"message": "%{msg}",
									"request_type": "%{method}",
									"remote_api_ver": "%{version}",
									"endpoint": "%{endpoint}",
									"urlpath": "%{urlpath}",
									"response_msg": "%{doc_msg}",
									"processing_time": "%{processing_time}"
								} ]
							} ]
						}'
					}
				}
			}
			else {

				grok {
					patterns_dir => "/logcollector_config/patterns/"
					match => [ "[body][syslog_message]", "%{SYSLOG_MSG_1}", "[body][syslog_message]", "%{SYSLOG_MSG_2}", "[body][syslog_message]", "%{SYSLOG_MSG_COMMON}" ]
				}

				if ![origin_timestamp] {
					mutate {
						add_field => {
							"temp_origin_timestamp" => "%{temp_timestamp}"
						}
					}
				} else {
					mutate {
						update => [ "[body][syslog_message]", "%{origin_syslog_msg}" ]
					}
					if [origin_syslog_timezone] {
						mutate {
							add_field => [ "origin_timestamp_tz", "%{origin_timestamp} %{origin_syslog_timezone}" ]
						}
						date {
							match => [ "origin_timestamp_tz", "MMM  d HH:mm:ss ZZZ", "MMM dd HH:mm:ss ZZZ" ]
							timezone => "%{origin_syslog_timezone}"
							target => "origin_timestamp_tz"
						}
						mutate {
							update => { "origin_timestamp" => "%{origin_timestamp_tz}" }
						}
					} else {
						date {
							match => [ "origin_timestamp", "MMM  d HH:mm:ss.SSS", "MMM dd HH:mm:ss.SSS" ]
							timezone => "UTC"
							target => "origin_timestamp"
						}
					}
					mutate {
						add_field => {
							"temp_origin_timestamp" => "%{origin_timestamp}"
						}
					}
					mutate {
						gsub => [
							"temp_origin_timestamp", "\.\d{3}Z$", "Z"
						]
					}
				}

				# jarvis 2.0 header & body
				mutate {
					add_field => {
						"jarvis_message" => '{
							"documents": [ {
								"header": {
									"product_id": "ao",
									"tenant_id": "%{[body][tenant_id]}",
									"doc_type_id": "itoa_logs_syslog",
									"doc_type_version": "1"
								},
								"body": [ {
									"timestamp": "%{temp_timestamp}",
									"received_timestamp": "%{temp_received_timestamp}",
									"logtype": "syslog",
									"syslog_timestamp": "%{temp_timestamp}",
									"syslog_pri": "%{[body][syslog_pri]}",
									"syslog_message": "%{[body][syslog_message]}",
									"host": "%{[body][host]}",
									"syslog_severity": "%{[body][syslog_severity]}",
									"syslog_facility": "%{[body][syslog_facility]}",
									"syslog_severity_code": %{[body][syslog_severity_code]},
									"syslog_facility_code": %{[body][syslog_facility_code]},
									"syslog_program": "%{[body][syslog_program]}",
									"syslog_pid": "%{[body][syslog_pid]}",
									"syslog_hostname": "%{[body][syslog_hostname]}",
									"syslog_priority": %{[body][syslog_priority]},
									"origin_timestamp": "%{temp_origin_timestamp}",
									"origin_facility_code": "%{origin_facility_code}",
									"origin_message_number": "%{origin_message_number}"
								} ]
							} ]
						}'
					}
				}
			}


			#######################################################################################
			# Any parsing failures (Grok and Json) for supported log types goes to Generic Index  #
			#######################################################################################
			if (("_grokparsefailure" in [tags] or "_jsonparsefailure" in [tags]) and [type] != "syslog") {

				mutate {
					remove_field => [ "temp_timestamp", "temp_received_timestamp", "jarvis_message" ]
				}

				mutate {
					add_field => {
						"temp_timestamp" => "%{@timestamp}"
						"temp_received_timestamp" => "%{@timestamp}"
					}
				}

				mutate {
					gsub => [
						"temp_timestamp", "\.\d{3}Z$", "Z",
						"temp_received_timestamp", "\.\d{3}Z$", "Z"
					]
				}

				# Handling new line character, tabs etc
				mutate {
					gsub => ['message', "[\\]", "/"]
					gsub => ['message', "\"", ""]
				}
				# jarvis 2.0 header & body
				mutate {
					add_field => {
						"jarvis_message" => '{
							"documents": [ {
								"header": {
									"product_id": "ao",
									"tenant_id": "%{tenant_id}",
									"doc_type_id": "itoa_logs_generic",
									"doc_type_version": "1"
								},
								"body": [ {
									"timestamp": "%{temp_timestamp}",
									"received_timestamp": "%{temp_received_timestamp}",
									"logtype": "%{type}",
									"host": "%{sender_hostname}",
									"ip": "%{sender_ipaddress}",
									"tags": ["%{sender_tags}, %{tags}"],
									"file": "%{sender_filename}",
									"origin": "%{origin}",
									"message": "%{message}"
								} ]
							} ]
						}'
					}
				}
			}

			# Global settings for string and numeric fields in JSON
			mutate {
				gsub => [
					"jarvis_message"  , "\"%{.*?}\"", '""',
					"jarvis_message"  , "%{.*?}", 0
				]
			}

	  }
  }



	# DO NOT Use this in production.
	prune {
		interpolate => true
		whitelist_names => [ "jarvis_message", "message", "tags", "missing_fields", "auth_token" ]
	}

	# Enable it for Logstash Performance Metrices
	metrics {
		meter => "documents"
		add_tag => "metric"
		flush_interval => 60
	}
	# DO NOT use this json parsing on PRODUCTION
	# Enable it only for local testing or analyzing parsing failures
	#json {
	#	source => "jarvis_message"
	#	target => "jarvis_json"
	#}

}

output {

	# Enable it for Logstash Performance Metrices
	if "metric" in [tags] {
		stdout {
			codec => line {
				format => "Logstash Rate: %{[documents][rate_1m]}, Count: %{[documents][count]}"
			}
		}
    }

	else if "_ingestionfailure" in [tags] {
		stdout {
			codec => line { format => "Ingestion failure: Missing mandatory fields: %{missing_fields}; Message: %{message}

			" }
		}
	}

	else {
		# Writing logs output to OI SaaS Jarvis Log ingestion endpoint
		if ( ([@metadata][ingestIntoJarvis] == 'true') )  {

			http {
				http_method => "post"
				url => "/ingestion"
				format => "message"
				headers => {"Content-Type" => "application/json"}
				####headers => {"Authorization" => "Basic %{auth_token}"}
				content_type => "application/json"
				#message => '{"documents": [{ "header":  %{header} ,"body": [%{body}] }]}'
				message => '%{jarvis_message}'
				pool_max => 100
				pool_max_per_route => 100
				#workers => 4
				#proxy => "http://${PROXYHOST}:${PROXYPORT}"
				##=proxy => {
                                        #host => "${PROXYHOST}"
                                        #port => "${PROXYPORT}"
                                        #scheme => 'http'
                                ####}
			}
		}else {
			# Writing logs output to OI SaaS Ingestion API Log endpoint
			http {
				http_method => "post"
				#url => "https://logcollector-route-6060-ao-doi.app.gpus1.saas.broadcom.com/mdo/v2/aoanalytics/ingestion/syslog"
				url => "https://logcollector.dxi-na1.saas.broadcom.com/mdo/v2/aoanalytics/ingestion/syslog"
				format => "message"
				headers => {"Content-Type" => "application/json"}
				####headers => {"Authorization" => "Basic %{auth_token}"}
				content_type => "application/json"
				message  => '%{message}'
				pool_max => 1000
				pool_max_per_route => 100
				#workers => 4
				#proxy => "http://${PROXYHOST}:${PROXYPORT}"
				##=proxy => {
                                        #host => "${PROXYHOST}"
                                        #port => "${PROXYPORT}"
                                        #scheme => 'http'
                                ####}
			 }
		}
	}
	# DO NOT enable rubydebug in production as it significantly degrades performance
	#if "tenant_not_found" not in [tags] {
		stdout {
			codec => rubydebug
		}
	#}

}
